#!/usr/bin/env python3
"""
OPTIMIZED SDK STRESS TEST
========================
Test our performance optimizations against the backend performance crisis
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'sdk', 'python'))

from jeanmemory import JeanClient
import time
import concurrent.futures
import json
from datetime import datetime

def test_single_request(client, test_id, message):
    """Test a single optimized request"""
    start_time = time.time()
    
    try:
        response = client.get_context(
            user_token=f'stress_test_user_{test_id}',
            message=message,
            is_new_conversation=False
        )
        
        duration = time.time() - start_time
        return {
            'test_id': test_id,
            'success': True,
            'duration': duration,
            'response_size': len(response.text),
            'attempts_visible': 'first_attempt' if duration < 5 else 'retry_attempts'
        }
        
    except Exception as e:
        duration = time.time() - start_time
        return {
            'test_id': test_id,
            'success': False,
            'duration': duration,
            'error': str(e)
        }

def main():
    print("üöÄ OPTIMIZED SDK STRESS TEST")
    print("Testing our performance optimizations against backend issues")
    print()
    
    # Create optimized client
    client = JeanClient('jean_sk_f3LqQ_2KMDLlD681e7cTEHAhMyhDXdbvct-cZR6Ryrk')
    
    # Test scenarios
    messages = [
        "Optimized stress test - what do you know about me?",
        "Testing retry logic and connection pooling",
        "SDK optimization verification in progress",
        "Performance improvement validation test",
        "Exponential backoff stress testing",
        "Connection reuse and timeout optimization test"
    ]
    
    # Run tests with moderate concurrency (don't overload our own optimizations)
    num_tests = 50
    max_workers = 10  # Reduced from chaos testing to be more realistic
    
    print(f"üî• Launching {num_tests} optimized requests with {max_workers} workers")
    print()
    
    results = []
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        
        for i in range(num_tests):
            message = messages[i % len(messages)]
            future = executor.submit(test_single_request, client, i + 1, message)
            futures.append(future)
        
        # Collect results as they complete
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            results.append(result)
            
            status = "‚úÖ" if result['success'] else "‚ùå"
            duration = result['duration']
            
            if result['success']:
                size_info = f" ({result['response_size']} chars)"
                attempts = result['attempts_visible']
                print(f"{status} Test {result['test_id']}: {duration:.3f}s{size_info} [{attempts}]")
            else:
                print(f"{status} Test {result['test_id']}: {duration:.3f}s - {result['error'][:100]}")
    
    # Generate report
    total_duration = time.time() - start_time
    success_count = sum(1 for r in results if r['success'])
    fail_count = num_tests - success_count
    success_rate = (success_count / num_tests) * 100
    
    successful_results = [r for r in results if r['success']]
    avg_duration = sum(r['duration'] for r in successful_results) / len(successful_results) if successful_results else 0
    
    response_sizes = [r['response_size'] for r in successful_results if 'response_size' in r]
    avg_response_size = sum(response_sizes) / len(response_sizes) if response_sizes else 0
    
    first_attempt_success = sum(1 for r in successful_results if r.get('attempts_visible') == 'first_attempt')
    retry_success = success_count - first_attempt_success
    
    print()
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë                    üöÄ OPTIMIZED SDK STRESS REPORT üöÄ                      ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print()
    print(f"‚è±Ô∏è  Total Duration: {total_duration:.2f}s")
    print(f"üéØ Total Tests: {num_tests}")
    print(f"‚úÖ Successful: {success_count} ({success_rate:.1f}%)")
    print(f"‚ùå Failed: {fail_count} ({(fail_count/num_tests)*100:.1f}%)")
    print()
    print("üìä PERFORMANCE METRICS:")
    print(f"   ‚Ä¢ Avg Success Duration: {avg_duration:.3f}s")
    print(f"   ‚Ä¢ Avg Response Size: {avg_response_size:.0f} characters")
    print(f"   ‚Ä¢ Requests Per Second: {num_tests/total_duration:.1f} RPS")
    print()
    print("üîÑ RETRY ANALYSIS:")
    print(f"   ‚Ä¢ First Attempt Success: {first_attempt_success}")
    print(f"   ‚Ä¢ Retry Logic Success: {retry_success}")
    print(f"   ‚Ä¢ Retry Effectiveness: {(retry_success/success_count)*100:.1f}% of successes needed retries" if success_count > 0 else "   ‚Ä¢ No successful requests to analyze")
    print()
    
    # Compare to previous chaos testing results
    print("üìà COMPARISON TO UNOPTIMIZED CHAOS TESTS:")
    previous_backend_success_rate = 9.0  # From backend hammer
    previous_nodejs_success_rate = 2.0   # From Node.js stress test
    
    print(f"   ‚Ä¢ Backend Hammer (unoptimized): {previous_backend_success_rate:.1f}% success")
    print(f"   ‚Ä¢ Node.js Stress (unoptimized): {previous_nodejs_success_rate:.1f}% success")
    print(f"   ‚Ä¢ Optimized Python SDK: {success_rate:.1f}% success")
    
    if success_rate > previous_backend_success_rate:
        improvement = success_rate - previous_backend_success_rate
        print(f"   ‚Ä¢ üéâ IMPROVEMENT: +{improvement:.1f}% vs backend hammer!")
    
    if success_rate > previous_nodejs_success_rate:
        improvement = success_rate - previous_nodejs_success_rate
        print(f"   ‚Ä¢ üéâ IMPROVEMENT: +{improvement:.1f}% vs Node.js stress!")
    
    print()
    
    # Final verdict
    if success_rate >= 80:
        verdict = "üéâ OPTIMIZATIONS SUCCESSFUL - PRODUCTION READY!"
    elif success_rate >= 50:
        verdict = "‚ö° SIGNIFICANT IMPROVEMENT - NEEDS MINOR TUNING"
    elif success_rate > max(previous_backend_success_rate, previous_nodejs_success_rate):
        verdict = "üìà OPTIMIZATIONS WORKING - MAJOR BACKEND ISSUES REMAIN"
    else:
        verdict = "üö® OPTIMIZATIONS INSUFFICIENT - BACKEND CRISIS CONTINUES"
    
    print(f"üèÅ FINAL VERDICT: {verdict}")
    
    # Save detailed results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f'./logs/optimized_stress_test_{timestamp}.json'
    
    try:
        os.makedirs('./logs', exist_ok=True)
        with open(report_path, 'w') as f:
            json.dump({
                'metadata': {
                    'timestamp': timestamp,
                    'total_duration': total_duration,
                    'total_tests': num_tests,
                    'success_rate': success_rate,
                    'avg_duration': avg_duration,
                    'avg_response_size': avg_response_size,
                    'first_attempt_success': first_attempt_success,
                    'retry_success': retry_success
                },
                'results': results
            }, f, indent=2)
        
        print(f"üìÑ Detailed report saved: {report_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save report: {e}")

if __name__ == "__main__":
    main()