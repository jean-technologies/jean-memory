# Smart Context Performance Issue - Debug Status

## üö® **CRITICAL LATENCY ISSUE IDENTIFIED**

Date: Jan 26, 2025  
Status: **UNUSABLE DUE TO PERFORMANCE**  
Branch: `feature/smart-context-orchestration`

## ‚úÖ **What's Working**

All legacy memory tools are functioning perfectly with good performance:

- ‚úÖ **`list_memories`** - Fast, returns all stored memories
- ‚úÖ **`search_memory`** - Fast, semantic search with scores  
- ‚úÖ **`ask_memory`** - Fast (1.84s total), natural language answers
- ‚úÖ **`add_memories`** - Fast, stores new memories
- ‚úÖ **`deep_memory_query`** - Slow but acceptable (6.98s), comprehensive analysis

## ‚ùå **What's Broken**

- ‚ùå **`smart_context`** - **UNUSABLE LATENCY** - Hangs indefinitely, never returns

## üîç **Performance Analysis**

### Legacy Tools Performance
```
ask_memory:      1.84s (search=0.66s, llm=0.99s)
deep_memory_query: 6.98s (mem_fetch=0.89s, gemini=5.99s)
smart_context:   ‚àû (HANGS - NEVER RETURNS)
```

### Root Cause Hypotheses
1. **Gemini API Bottleneck**: Multiple sequential AI calls causing timeouts
2. **Infinite Loop**: AI reasoning getting stuck in recursive analysis
3. **Blocking Operations**: Synchronous operations blocking async execution
4. **Memory Overhead**: Working memory + AI analysis consuming too much time
5. **Network Timeouts**: Long-running Gemini API calls exceeding limits

## üß™ **Test Results Summary**

**User ID**: `00000000-0000-0000-0000-000000000001`  
**Backend**: Running on `http://localhost:8765`  
**Claude Desktop**: Connected via MCP

**Test Query**: `"What memory tools do you see"`
- **Expected**: Quick context analysis with tool information
- **Actual**: Complete hang, no response, had to cancel

## üîß **IMMEDIATE DEBUGGING STEPS**

### 1. Add Comprehensive Logging

**File**: `openmemory/api/app/mcp_orchestration.py`

Add detailed timing and status logs:

```python
import time
import logging

logger = logging.getLogger(__name__)

async def orchestrate_smart_context(self, user_message: str, conversation_context: Optional[str] = None, user_id: str, client_name: str = "unknown") -> str:
    start_time = time.time()
    logger.info(f"üöÄ [SMART_CONTEXT] Starting orchestration for user {user_id}")
    
    try:
        # Log each major step
        step_start = time.time()
        session_cache = self._check_session_cache(cache_key)
        logger.info(f"‚è±Ô∏è [SMART_CONTEXT] Cache check: {time.time() - step_start:.2f}s")
        
        step_start = time.time()
        conversation_state = await self._detect_new_conversation(user_message, working_memory)
        logger.info(f"‚è±Ô∏è [SMART_CONTEXT] Conversation detection: {time.time() - step_start:.2f}s - Result: {conversation_state}")
        
        step_start = time.time()  
        memory_analysis = await self._ai_memory_analysis(user_message, conversation_context)
        logger.info(f"‚è±Ô∏è [SMART_CONTEXT] Memory analysis: {time.time() - step_start:.2f}s - Memorable: {memory_analysis.get('is_memorable', False)}")
        
        # Continue logging each step...
        
    except Exception as e:
        logger.error(f"‚ùå [SMART_CONTEXT] Error after {time.time() - start_time:.2f}s: {e}", exc_info=True)
        raise
    finally:
        logger.info(f"üèÅ [SMART_CONTEXT] Total execution time: {time.time() - start_time:.2f}s")
```

### 2. Add Gemini API Logging

**File**: `openmemory/api/app/utils/gemini.py`

```python
async def generate_response(self, prompt: str, response_format: str = "text") -> Union[str, Dict]:
    start_time = time.time()
    logger.info(f"ü§ñ [GEMINI] Starting API call - Format: {response_format}")
    logger.debug(f"ü§ñ [GEMINI] Prompt length: {len(prompt)} chars")
    
    try:
        response = await self.model.generate_content_async(prompt)
        logger.info(f"‚è±Ô∏è [GEMINI] API call completed: {time.time() - start_time:.2f}s")
        
        if response.candidates[0].finish_reason == 2:
            logger.warning(f"‚ö†Ô∏è [GEMINI] Safety filter triggered")
            
        return response.text
        
    except Exception as e:
        logger.error(f"‚ùå [GEMINI] API call failed after {time.time() - start_time:.2f}s: {e}")
        raise
```

### 3. Test Individual Components

**File**: `openmemory/api/debug_smart_context.py`

```python
#!/usr/bin/env python3
"""
Debug script to test Smart Context components individually
"""
import asyncio
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from app.mcp_orchestration import SmartContextOrchestrator
from app.utils.gemini import GeminiService

async def test_components():
    orchestrator = SmartContextOrchestrator()
    
    # Test 1: Gemini API basic call
    print("üß™ Testing Gemini API...")
    start = time.time()
    try:
        result = await orchestrator.gemini_service.generate_response("Hello, respond with 'OK'")
        print(f"‚úÖ Gemini basic test: {time.time() - start:.2f}s - Result: {result}")
    except Exception as e:
        print(f"‚ùå Gemini basic test failed: {e}")
    
    # Test 2: Conversation detection
    print("üß™ Testing conversation detection...")
    start = time.time()
    try:
        result = await orchestrator._detect_new_conversation("Hello", [])
        print(f"‚úÖ Conversation detection: {time.time() - start:.2f}s - Result: {result}")
    except Exception as e:
        print(f"‚ùå Conversation detection failed: {e}")
    
    # Test 3: Memory analysis
    print("üß™ Testing memory analysis...")
    start = time.time()
    try:
        result = await orchestrator._ai_memory_analysis("I like Python programming", None)
        print(f"‚úÖ Memory analysis: {time.time() - start:.2f}s - Result: {result}")
    except Exception as e:
        print(f"‚ùå Memory analysis failed: {e}")

if __name__ == "__main__":
    asyncio.run(test_components())
```

### 4. Enable Debug Logging

**Run the backend with debug logging**:

```bash
cd openmemory/api
export LOG_LEVEL=DEBUG
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8765 --log-level debug
```

### 5. Monitor Resource Usage

```bash
# Terminal 1: Watch memory usage
watch -n 1 "ps aux | grep uvicorn"

# Terminal 2: Watch network connections  
watch -n 1 "netstat -an | grep 8765"

# Terminal 3: Monitor backend logs
tail -f openmemory/api/logs/app.log
```

## üéØ **PRIORITIZED INVESTIGATION PLAN**

### Phase 1: Identify Bottleneck (URGENT)
1. ‚úÖ Add comprehensive logging (above)
2. ‚úÖ Run individual component tests
3. ‚úÖ Identify which specific operation is hanging
4. ‚úÖ Check Gemini API rate limits/quotas

### Phase 2: Quick Fixes (HIGH PRIORITY)
1. **Add Timeouts**: Prevent infinite hangs
   ```python
   async with asyncio.timeout(30):  # 30 second timeout
       result = await self._detect_new_conversation(...)
   ```

2. **Optimize AI Calls**: Reduce sequential API calls
   ```python
   # Instead of sequential:
   conv_state = await self._detect_new_conversation(...)
   memory_analysis = await self._ai_memory_analysis(...)
   
   # Do parallel:
   conv_task = self._detect_new_conversation(...)
   memory_task = self._ai_memory_analysis(...)
   conv_state, memory_analysis = await asyncio.gather(conv_task, memory_task)
   ```

3. **Simplify AI Prompts**: Reduce token count and complexity

### Phase 3: Performance Optimization (MEDIUM PRIORITY)
1. **Cache AI Responses**: Cache common AI reasoning patterns
2. **Batch Operations**: Combine multiple AI calls into single requests
3. **Background Processing**: Move non-critical operations to background
4. **Streaming Responses**: Return partial results while processing

## üöÄ **QUICK WIN OPPORTUNITIES**

### Option A: Add Timeout + Fallback
```python
async def orchestrate_smart_context(self, ...):
    try:
        async with asyncio.timeout(10):  # 10 second timeout
            return await self._full_orchestration(...)
    except asyncio.TimeoutError:
        logger.warning("Smart context timed out, falling back to simple search")
        return await self._simple_fallback_search(user_message, user_id)
```

### Option B: Staged Rollout
```python
# Add feature flag for smart context
ENABLE_SMART_CONTEXT = os.getenv("ENABLE_SMART_CONTEXT", "false").lower() == "true"

async def smart_context(...):
    if not ENABLE_SMART_CONTEXT:
        return await self._legacy_search_and_add(...)
    return await self._full_smart_context(...)
```

## üìä **SUCCESS METRICS**

Target performance after fixes:
- ‚úÖ **`smart_context`**: < 5 seconds for simple queries
- ‚úÖ **`smart_context`**: < 10 seconds for complex queries  
- ‚úÖ **No hangs**: Always returns within timeout period
- ‚úÖ **Graceful fallback**: If AI fails, fall back to legacy tools

## üî• **NEXT STEPS FOR DEBUGGER**

1. **Run the logging setup** (above code changes)
2. **Execute the debug script** to test individual components
3. **Test smart_context with debug logs** and identify the hanging operation
4. **Implement timeout + fallback** as immediate fix
5. **Optimize the identified bottleneck**
6. **Test performance improvements**
7. **Consider production deployment** only after < 5s response times

## üìÅ **Files to Modify**

- `openmemory/api/app/mcp_orchestration.py` - Add logging
- `openmemory/api/app/utils/gemini.py` - Add API logging  
- `openmemory/api/debug_smart_context.py` - New debug script
- `openmemory/api/app/mcp_server.py` - Add timeout to smart_context tool

## üéØ **Expected Outcome**

After implementing these changes:
1. **Identify the root cause** of the latency (likely sequential Gemini API calls)
2. **Implement immediate fix** (timeouts + fallback)
3. **Optimize performance** (parallel calls, caching)
4. **Achieve target performance** (< 5s response times)
5. **Deploy to production** with confidence

The Smart Context orchestration concept is sound - the implementation just needs performance optimization to be production-ready. 