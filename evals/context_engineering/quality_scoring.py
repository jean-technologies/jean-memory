#!/usr/bin/env python3
"""
Context Quality Scoring Evaluation
Tests the quality of context generated by Jean Memory's orchestrator
"""

import asyncio
import json
import logging
import argparse
from typing import Dict, List, Any
from pathlib import Path
import sys
import os

# Add project paths
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root / "openmemory" / "api"))
sys.path.insert(0, str(current_dir.parent / "utils"))

from eval_framework import BaseEvaluator, TestScenario, EvalResult, create_test_user_context
from metrics import ContextQualityEvaluator, ContextQualityScore

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ContextQualityScoringEvaluator(BaseEvaluator):
    """Evaluates context quality using multiple scoring metrics"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("context_quality_scoring", config)
        self.quality_evaluator = ContextQualityEvaluator()
        self.quality_threshold = config.get('quality_threshold', 75.0) if config else 75.0
        
    async def run_evaluation(self, scenarios: List[TestScenario]) -> List[EvalResult]:
        """Run context quality evaluation on all scenarios"""
        logger.info(f"Starting context quality evaluation with {len(scenarios)} scenarios")
        
        results = []
        for scenario in scenarios:
            result = await self.run_single_scenario(scenario)
            results.append(result)
        
        # Log summary
        passed = sum(1 for r in results if r.passed)
        avg_score = sum(r.score for r in results) / len(results) if results else 0
        
        logger.info(f"Context quality evaluation complete: {passed}/{len(results)} passed, avg score: {avg_score:.1f}")
        return results
    
    async def _execute_scenario(self, scenario: TestScenario) -> Dict[str, Any]:
        """Execute context quality evaluation for a single scenario"""
        try:
            # Extract input data
            user_message = scenario.input_data['user_message']
            user_id = scenario.input_data.get('user_id', 'eval-test-user')
            client_name = scenario.input_data.get('client_name', 'eval-test-client')
            is_new_conversation = scenario.input_data.get('is_new_conversation', False)
            
            # Get context from orchestrator
            context_result = await self._get_orchestrator_context(
                user_message, user_id, client_name, is_new_conversation
            )
            
            if not context_result['success']:
                return {
                    'error': context_result.get('error', 'Unknown error'),
                    'context': '',
                    'quality_score': ContextQualityScore(0, 0, 0, 100, 0).to_dict(),
                    'response_time': context_result.get('response_time', 0)
                }
            
            # Evaluate context quality
            context = context_result['context']
            expected_elements = scenario.expected_output.get('expected_elements', [])
            user_profile = scenario.input_data.get('user_profile', {})
            
            quality_score = self.quality_evaluator.evaluate_context_quality(
                context=context,
                user_query=user_message,
                expected_elements=expected_elements,
                user_profile=user_profile
            )
            
            return {
                'context': context,
                'quality_score': quality_score.to_dict(),
                'response_time': context_result['response_time'],
                'success': True,
                'context_length': len(context),
                'contains_expected_elements': self._check_expected_elements(context, expected_elements)
            }
            
        except Exception as e:
            logger.error(f"Error executing scenario {scenario.id}: {e}")
            return {
                'error': str(e),
                'context': '',
                'quality_score': ContextQualityScore(0, 0, 0, 100, 0).to_dict(),
                'response_time': 0
            }
    
    def calculate_score(self, actual: Dict[str, Any], expected: Dict[str, Any]) -> float:
        """Calculate score based on context quality metrics"""
        if 'error' in actual:
            return 0.0
        
        quality_score = actual.get('quality_score', {})
        overall_score = quality_score.get('overall', 0.0)
        
        # Apply bonus/penalty based on specific criteria
        response_time = actual.get('response_time', 0)
        if response_time > 5.0:  # Response took too long
            overall_score *= 0.9  # 10% penalty
        
        expected_score = expected.get('min_quality_score', self.quality_threshold)
        if overall_score >= expected_score:
            return overall_score
        else:
            # Partial credit for scores below threshold
            return overall_score * 0.8
    
    async def _get_orchestrator_context(self, user_message: str, user_id: str, 
                                      client_name: str, is_new_conversation: bool) -> Dict[str, Any]:
        """Get context from the actual orchestrator"""
        try:
            # Import here to avoid circular dependencies
            from app.mcp_orchestration import get_smart_orchestrator
            from app.context import user_id_var, client_name_var
            import time
            
            # Set context variables
            user_token = user_id_var.set(user_id)
            client_token = client_name_var.set(client_name)
            
            try:
                orchestrator = get_smart_orchestrator()
                
                start_time = time.time()
                context = await orchestrator.orchestrate_smart_context(
                    user_message=user_message,
                    user_id=user_id,
                    client_name=client_name,
                    is_new_conversation=is_new_conversation
                )
                end_time = time.time()
                
                return {
                    'context': context or '',
                    'response_time': end_time - start_time,
                    'success': True
                }
                
            finally:
                # Reset context variables
                user_id_var.reset(user_token)
                client_name_var.reset(client_token)
                
        except Exception as e:
            logger.error(f"Error getting orchestrator context: {e}")
            return {
                'context': '',
                'response_time': 0.0,
                'success': False,
                'error': str(e)
            }
    
    def _check_expected_elements(self, context: str, expected_elements: List[str]) -> Dict[str, bool]:
        """Check which expected elements are present in the context"""
        context_lower = context.lower()
        return {
            element: element.lower() in context_lower
            for element in expected_elements
        }

def create_context_quality_scenarios() -> List[TestScenario]:
    """Create test scenarios for context quality evaluation"""
    
    scenarios = [
        TestScenario(
            id="new_conversation_software_engineer",
            description="New conversation with software engineer introduction",
            input_data={
                'user_message': "Hi! I'm Jonathan, a software engineer working on AI projects. I love building innovative solutions.",
                'user_id': 'eval-test-se-001',
                'client_name': 'claude',
                'is_new_conversation': True,
                'user_profile': {
                    'profession': 'software_engineer',
                    'interests': ['AI', 'innovation', 'building solutions'],
                    'preferences': ['clean architecture', 'TypeScript']
                }
            },
            expected_output={
                'expected_elements': [
                    'software engineer',
                    'AI projects', 
                    'innovative solutions',
                    'building'
                ],
                'min_quality_score': 80.0,
                'expected_strategy': 'new_conversation'
            },
            success_criteria={
                'min_score': 80.0,
                'contains_elements': ['software', 'engineer', 'AI'],
                'response_time': 5.0
            },
            tags=['new_conversation', 'professional', 'technical']
        ),
        
        TestScenario(
            id="continuing_conversation_python_help",
            description="Continuing conversation asking for Python help",
            input_data={
                'user_message': "Can you help me optimize this Python function for better performance?",
                'user_id': 'eval-test-py-001',
                'client_name': 'claude',
                'is_new_conversation': False,
                'user_profile': {
                    'profession': 'software_engineer',
                    'preferences': ['Python', 'performance optimization', 'clean code'],
                    'recent_projects': ['web scraper', 'data analysis tool']
                }
            },
            expected_output={
                'expected_elements': [
                    'Python',
                    'performance',
                    'optimization',
                    'software engineer'
                ],
                'min_quality_score': 75.0,
                'expected_strategy': 'relevant_context'
            },
            success_criteria={
                'min_score': 75.0,
                'contains_elements': ['Python', 'performance'],
                'response_time': 3.0
            },
            tags=['continuing_conversation', 'technical', 'python']
        ),
        
        TestScenario(
            id="personal_query_career_goals",
            description="Personal query about career goals",
            input_data={
                'user_message': "What are my main career goals and how should I prioritize them?",
                'user_id': 'eval-test-career-001',
                'client_name': 'claude',
                'is_new_conversation': False,
                'user_profile': {
                    'profession': 'product_manager',
                    'interests': ['leadership', 'strategy', 'innovation'],
                    'goals': ['become VP', 'launch successful product', 'build great team']
                }
            },
            expected_output={
                'expected_elements': [
                    'career goals',
                    'VP',
                    'product',
                    'leadership'
                ],
                'min_quality_score': 85.0,
                'expected_strategy': 'comprehensive_analysis'
            },
            success_criteria={
                'min_score': 85.0,
                'contains_elements': ['career', 'goals'],
                'response_time': 5.0
            },
            tags=['personal', 'career', 'comprehensive']
        ),
        
        TestScenario(
            id="no_context_needed_general_query",
            description="General knowledge query that doesn't need personal context",
            input_data={
                'user_message': "What is the relationship between introversion and conformity?",
                'user_id': 'eval-test-general-001', 
                'client_name': 'claude',
                'is_new_conversation': False,
                'needs_context': False
            },
            expected_output={
                'expected_elements': [],
                'min_quality_score': 0.0,  # Should return "no context needed"
                'expected_response': 'Context is not required'
            },
            success_criteria={
                'not_contains': ['user', 'your', 'personal'],
                'contains_elements': ['context is not required'],
                'response_time': 1.0
            },
            tags=['no_context', 'general_knowledge']
        ),
        
        TestScenario(
            id="new_conversation_no_context_with_narrative",
            description="New conversation with general query but should get life narrative",
            input_data={
                'user_message': "What's the weather like today?",
                'user_id': 'eval-test-weather-001',
                'client_name': 'claude', 
                'is_new_conversation': True,
                'needs_context': False
            },
            expected_output={
                'expected_elements': ['life context', 'narrative'],
                'min_quality_score': 50.0,
                'expected_response': 'Context is not required'
            },
            success_criteria={
                'contains_elements': ['context is not required', 'life context'],
                'response_time': 3.0
            },
            tags=['new_conversation', 'no_context', 'life_narrative']
        )
    ]
    
    return scenarios

async def main():
    """Main evaluation function"""
    parser = argparse.ArgumentParser(description='Run context quality scoring evaluation')
    parser.add_argument('--user-id', default='eval-test-user', help='Test user ID')
    parser.add_argument('--scenarios-file', help='Path to custom scenarios JSON file')
    parser.add_argument('--output-file', help='Path to save results JSON file')
    parser.add_argument('--quality-threshold', type=float, default=75.0, help='Quality score threshold')
    
    args = parser.parse_args()
    
    # Load scenarios
    if args.scenarios_file:
        try:
            from eval_framework import load_test_scenarios
            scenarios = load_test_scenarios(args.scenarios_file)
        except Exception as e:
            logger.error(f"Failed to load scenarios from {args.scenarios_file}: {e}")
            return
    else:
        scenarios = create_context_quality_scenarios()
    
    # Configure evaluator
    config = {
        'quality_threshold': args.quality_threshold,
        'user_id': args.user_id
    }
    
    evaluator = ContextQualityScoringEvaluator(config)
    
    # Run evaluation
    logger.info("Starting context quality scoring evaluation...")
    results = await evaluator.run_evaluation(scenarios)
    
    # Print summary
    summary = evaluator.get_summary_stats()
    print("\n" + "="*60)
    print("CONTEXT QUALITY SCORING EVALUATION RESULTS")
    print("="*60)
    print(f"Total Tests: {summary['total_tests']}")
    print(f"Passed: {summary['passed']} ({summary['pass_rate']:.1f}%)")
    print(f"Failed: {summary['failed']}")
    print(f"Average Score: {summary['average_score']:.1f}")
    print(f"Score Range: {summary['min_score']:.1f} - {summary['max_score']:.1f}")
    print(f"Average Execution Time: {summary['average_execution_time']:.2f}s")
    print(f"Total Execution Time: {summary['total_execution_time']:.2f}s")
    
    # Show detailed results
    print("\nDETAILED RESULTS:")
    print("-" * 60)
    for result in results:
        status = "âœ… PASS" if result.passed else "âŒ FAIL"
        print(f"{status} {result.test_name}: {result.score:.1f}/100 ({result.execution_time:.2f}s)")
        
        if result.details.get('quality_score'):
            qs = result.details['quality_score']
            print(f"    Relevance: {qs['relevance']:.1f}, Completeness: {qs['completeness']:.1f}")
            print(f"    Personalization: {qs['personalization']:.1f}, Noise Penalty: {qs['noise_penalty']:.1f}")
        
        if 'error' in result.details:
            print(f"    Error: {result.details['error']}")
        print()
    
    # Save results if requested
    if args.output_file:
        evaluator.save_results(args.output_file)
        print(f"Results saved to {args.output_file}")
    
    # Return exit code based on results
    if summary['pass_rate'] >= 80.0:  # 80% pass rate threshold
        print("ğŸ‰ Context quality evaluation PASSED!")
        return 0
    else:
        print("âš ï¸ Context quality evaluation FAILED - below 80% pass rate")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)