# MCP Tool Simplification Plan
## Replacing Complex Orchestration with Simple Depth Parameter

### Current Architecture Analysis

#### Current `jean_memory` Tool (orchestration.py:46-133)
```python
async def jean_memory(
    user_message: str, 
    is_new_conversation: bool, 
    needs_context: bool = True, 
    speed: str = "autonomous", 
    format: str = "enhanced"
) -> str:
```

**Current Speed Mode Routing:**
- `fast` â†’ `search_memory(limit=10)` 
- `balanced` â†’ `ask_memory()` (Gemini 2.5 Flash synthesis)
- `autonomous` â†’ Complex orchestration via `SmartContextOrchestrator` (1,800+ lines)
- `comprehensive` â†’ `deep_memory_query()` (documents + memory analysis)

#### Key Discovery: Memory Saving Logic
The **ONLY** critical functionality from the complex orchestration is:
1. **Background memory triage** - `triage_and_save_memory_background()` (lines 1735-1787)
2. **Content deduplication** - Prevents saving identical content multiple times

The complex orchestration does NOT handle whether memory should be saved - that's handled by background tasks that run regardless of the response path.

### Proposed Simplified Architecture 

#### New MCP Tool Schema
```python
async def jean_memory(
    user_message: str,
    is_new_conversation: bool,
    needs_context: bool = True,
    depth: int = 2,  # NEW: Simple depth parameter
    format: str = "enhanced"
) -> str:
```

**New Depth-Based Routing:**
- `depth=1` (fast) â†’ `search_memory(limit=10)`
- `depth=2` (balanced) â†’ `ask_memory()` 
- `depth=3` (comprehensive) â†’ `deep_memory_query()`

#### Preserved Critical Functionality
1. **Keep existing background memory saving** - No changes to `triage_and_save_memory_background()`
2. **Keep all working core tools** - `search_memory`, `ask_memory`, `deep_memory_query`
3. **Preserve API autonomous mode** - Keep complex orchestration for API users who want it

### Implementation Strategy

#### Phase 1: Create New MCP Tool (No Breaking Changes)
1. **Add new `jean_memory_depth` tool** alongside existing `jean_memory`
2. **Implement simple depth routing** in new tool
3. **Preserve all background memory saving logic**
4. **Test thoroughly** with existing Claude connections

#### Phase 2: Update Client Schemas (Controlled Rollout)
1. **Update `claude.py` client profile** to use new tool schema
2. **Keep existing tool as fallback** for compatibility
3. **Deploy to staging** for testing

#### Phase 3: Gradual Migration
1. **Update MCP server to serve new tool by default**
2. **Monitor for any breaking changes**
3. **Keep old tool available** for legacy connections

### Client Impact Analysis

#### Claude Desktop/Code Users
- **No action required** - MCP tools are dynamically fetched
- **Automatic upgrade** when we deploy new tool schema
- **Backward compatible** - existing conversations continue working

#### API Users (SDK, Virginia v2)
- **No impact** - Can still use `speed="autonomous"` for complex orchestration
- **New option available** - Can use `depth` parameter for simpler, faster responses

#### Other MCP Clients (Cursor, ChatGPT, etc.)
- **No breaking changes** - New tool parameter is optional
- **Default behavior preserved** - `depth=2` (balanced) matches current default

### Technical Implementation

#### File Changes Required
1. **`orchestration.py`** - Add new `jean_memory_depth` function
2. **`claude.py`** - Update tool schema to use `depth` parameter  
3. **`tool_registry.py`** - Register new tool function
4. **`mcp_instance.py`** - Add MCP decorator for new tool

#### Deployment Strategy
```bash
# Stage 1: Deploy new tool alongside existing (safe)
git checkout -b mcp-depth-tool
# Add jean_memory_depth tool
# Deploy to staging

# Stage 2: Update Claude client to use new tool
# Update claude.py schema
# Deploy to production

# Stage 3: Make new tool the default (after testing)
# Update tool_registry.py to map jean_memory -> jean_memory_depth
# Keep old function available as jean_memory_legacy
```

### Benefits of This Approach

#### Performance Improvements
- **Eliminates 1,800+ lines** of complex orchestration for MCP users
- **Reduces latency** by 2-5 seconds for most requests
- **Prevents empty responses** caused by orchestration failures

#### Maintains Flexibility
- **API users keep autonomous mode** for complex use cases
- **MCP users get simple, reliable experience**
- **All existing functionality preserved**

#### Zero Breaking Changes
- **Existing Claude connections** continue working
- **API backwards compatibility** maintained
- **Gradual migration** possible

### Risk Mitigation

#### Rollback Plan
1. **Keep old tool available** during transition
2. **Feature flag** to switch between old/new tool
3. **Monitoring** for any performance regressions

#### Testing Strategy
1. **Test with existing Claude Code connections**
2. **Verify all depth levels work correctly**
3. **Confirm memory saving still works**
4. **Load testing** with multiple clients

### Next Steps

1. âœ… **Analysis Complete** - Current architecture understood
2. ðŸ”„ **Create new tool** - Implement `jean_memory_depth` 
3. ðŸ”„ **Update client schema** - Modify `claude.py` tool definition
4. ðŸ”„ **Test thoroughly** - Verify no regressions
5. ðŸ”„ **Deploy gradually** - Staged rollout plan

### Questions Resolved

> **Will existing Claude users need to redownload?**
**No** - MCP tools are fetched dynamically. Users get the new tool automatically when we deploy.

> **Can we make this change without breaking existing implementations?**
**Yes** - We can add the new tool alongside the existing one, then gradually migrate.

> **What about memory saving functionality?**
**Preserved** - Background memory triage runs independently of the response orchestration.

> **Do we need to pack this into a new endpoint?**
**No** - We can deploy this as a simple tool schema update through existing MCP endpoints.

---

*This plan provides a path to dramatically simplify the MCP experience while preserving all critical functionality and maintaining complete backward compatibility.*